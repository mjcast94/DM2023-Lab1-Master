{"cells":[{"cell_type":"markdown","metadata":{"id":"FDNTc1wFid-1"},"source":["### Student Information\n","Name: Amanda Castellanos\n","\n","Student ID: 112065426\n","\n","GitHub ID: mjcast94"]},{"cell_type":"markdown","metadata":{"id":"-BajKEF_id-5"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"rSzgpPbVid-6"},"source":["### Instructions"]},{"cell_type":"markdown","metadata":{"id":"UjbdfsDcid-6"},"source":["1. First: do the **take home** exercises in the [DM2023-Lab1-Master](https://github.com/fjrialdnc0615/DM2023-Lab1-Master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n","\n","\n","2. Second: follow the same process from the [DM2023-Lab1-Master](https://github.com/fjrialdnc0615/DM2023-Lab1-Master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n","    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. You need to combine three labeled datasets into one file for your data preparation part.\n","    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n","\n","\n","3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n","    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n","    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n","    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n","\n","\n","4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n","\n","\n","5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n","\n","\n","You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fjrialdnc0615/DM2023-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Thursday)__. "]},{"cell_type":"markdown","metadata":{},"source":["Take Home Exercises"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":285,"status":"ok","timestamp":1666241724343,"user":{"displayName":"Kevin Yang","userId":"04518680380941289042"},"user_tz":-480},"id":"IDH6foBIid-9"},"outputs":[],"source":["### Begin Assignment Here\n","%load_ext autoreload\n","%autoreload 2\n","# categories\n","categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n","# obtain the documents containing the categories provided\n","from sklearn.datasets import fetch_20newsgroups\n","\n","twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n","twenty_train.data[0:3]\n","twenty_train.target_names\n","\n","#EXERCISE - 1 - Answer here\n","for t in twenty_train.data [:3]:\n","  print(t)\n","\n","import pandas as pd\n","\n","# my functions\n","import helpers.data_mining_helpers as dmh\n","\n","# construct dataframe from a list\n","X = pd.DataFrame.from_records(dmh.format_rows(twenty_train), columns= ['text'])\n","\n","# add category to the dataframe\n","X['category'] = twenty_train.target\n","\n","# add category label also\n","X['category_name'] = X.category.apply(lambda t: dmh.format_labels(t, twenty_train))\n","\n","#EXERCISE 2 - Answer here\n","\n","#every 3rd row\n","row3 = X.loc[X.index %3 == 0]\n","print(row3['text'])\n","\n","#category name for range\n","X.iloc[2000:2010, 2]\n","\n","#slicing and replacing value\n","e = X.copy()\n","print(e[:5])\n","e[:1] = 0\n","e[:5]\n","\n","#Exercise 3 - Answer here\n","X[X['category_name']== 'sci.med'].iloc[::10][:5]\n","\n","#Exercise 4 - Answer here\n","#X.isnull().apply(lambda y: [:], axis=1)\n","X.isnull().apply(lambda x: dmh.check_missing_values(x), axis=1)\n","\n","dummy_series = pd.Series([\"dummy_record\", 1], index=[\"text\", \"category\"])\n","dummy_series.to_frame().T\n","# .to_frame() -> Convert Series to DataFrame\n","# .T          -> Transpose\n","\n","result_with_series = pd.concat([X, dummy_series.to_frame().T], ignore_index=True)\n","\n","# check if the records was commited into result\n","len(result_with_series)\n","\n","result_with_series.isnull().apply(lambda x: dmh.check_missing_values(x))\n","#result_with_series.isnull().apply(lambda x: dmh.check_missing_values(x), axis=1)\n","\n","# dummy record as dictionary format\n","dummy_dict = [{'text': 'dummy_record','category': 1}]\n","\n","X = pd.concat([X, pd.DataFrame(dummy_dict)], ignore_index=True)\n","X.isnull().apply(lambda x: dmh.check_missing_values(x))\n","X.dropna(inplace=True)\n","X.isnull().apply(lambda x: dmh.check_missing_values(x))\n","\n","#EXERCISE 5\n","import numpy as np\n","\n","NA_dict = [{ 'id': 'A', 'missing_example': np.nan },\n","           { 'id': 'B'                    },\n","           { 'id': 'C', 'missing_example': 'NaN'  },\n","           { 'id': 'D', 'missing_example': 'None' },\n","           { 'id': 'E', 'missing_example':  None  },\n","           { 'id': 'F', 'missing_example': ''     }]\n","\n","NA_df = pd.DataFrame(NA_dict, columns = ['id','missing_example'])\n","NA_df\n","NA_df['missing_example'].isnull()\n","# Answer here\n","#is.null function is searching for EMPTY cells\n","#rows 0,1, and 4 each have values that equate to empty cells\n","#rows 2,3, and 5 technically are not empty as they contain text\n","\n","X.duplicated()\n","sum(X.duplicated())\n","sum(X.duplicated('text'))\n","dummy_duplicate_dict = [{\n","                             'text': 'dummy record',\n","                             'category': 1,\n","                             'category_name': \"dummy category\"\n","                        },\n","                        {\n","                             'text': 'dummy record',\n","                             'category': 1,\n","                             'category_name': \"dummy category\"\n","                        }]\n","\n","X = pd.concat([X, pd.DataFrame(dummy_duplicate_dict)], ignore_index=True)\n","len(X)\n","sum(X.duplicated())\n","X.drop_duplicates(keep=False, inplace=True) # inplace applies changes directly on our dataframe\n","len(X)\n","\n","\n","\n","print(X.shape)\n","X_sample = X.sample(n=1000,random_state=42) #random state\n","display(X_sample)\n","len(X_sample)\n","X_sample[0:4]\n","\n","#EXERCISE 6 - Answer\n","#the row and column sizes have been reduced\n","#rows were selected at random\n","\n","\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","categories\n","print(X.category_name.value_counts())\n","\n","# plot barchart for X\n","X.category_name.value_counts().plot(kind = 'bar',\n","                                    title = 'Category distribution',\n","                                    ylim = [0, 700],\n","                                    rot = 0, fontsize = 11, figsize = (8,3))\n","\n","print(X_sample.category_name.value_counts())\n","\n","# plot barchart for X_sample\n","X_sample.category_name.value_counts().plot(kind = 'bar',\n","                                           title = 'Category distribution',\n","                                           ylim = [0, 300],\n","                                           rot = 0, fontsize = 12, figsize = (8,3))\n","\n","\n","#EXERCISE 7 - Answer here\n","\n","# plot barchart for X_sample\n","print(max(X_sample.category_name.value_counts()))\n","upper_bound = max(X_sample.category_name.value_counts() + 50) #277 + 50\n","print(X_sample.category_name.value_counts())\n","\n","X_sample.category_name.value_counts().plot(kind = 'bar',\n","                                           title = 'Category distribution',\n","                                           ylim = [0, upper_bound],\n","                                           rot = 0, fontsize = 12, figsize = (8,3))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#EXERCISE 8 - Answer here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","X[0:4][\"unigrams\"]\n","X[0:4]\n","list(X[0:1]['unigrams'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","count_vect = CountVectorizer()\n","X_counts = count_vect.fit_transform(X.text) #learn the vocabulary and return document-term matrix\n","print(X_counts.shape)\n","print(X_counts[0])\n","count_vect.get_feature_names_out()[14887]\n","count_vect.get_feature_names_out()[29022]\n","count_vect.get_feature_names_out()[8696]\n","count_vect.get_feature_names_out()[4017]\n","analyze = count_vect.build_analyzer()\n","analyze(X.text[0])\n","\n","analyze = count_vect.build_analyzer()\n","analyze(\"I am craving for a hawaiian pizza right now\")\n","\n","# tokenization, remove stop words (e.g i, a, the), create n-gram (or unigram)\n","\n","#EXERCISE 9 - Answer here\n","# How do we turn our array[0] text document into a tokenized text using the build_analyzer()?\n","print(X.text[0])\n","analyze(X.text[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#EXERCISE 10 - Answer here\n","# :(\n","print(X_counts[5])\n","count_vect.get_feature_names_out()[14887]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#EXERCISE 11 - Answer here\n","#Instead of having the numerical values, we could display the vocab terms\n","#this is to make it more readable for the user\n","#Im still working on that exercise\n","\n","import seaborn as sns\n","df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n","display(df_todraw)\n","plt.subplots(figsize=(9, 7))\n","ax = sns.heatmap(df_todraw,\n","                 linewidths=2,\n","                 linecolor='purple',\n","                 cmap=\"PuRd\",\n","                 vmin=0, vmax=1, annot=True,\n","                 cbar=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#EXERCISE 12 - Answer here\n","#dimension reduction\n","print(X_counts.shape)\n","X_3red = PCA(n_components = 3).fit_transform(X_counts.toarray())\n","#print(X_3red.shape)\n","col = ['purple', 'green', 'orange', 'black']\n","#m = ['^', 'o', 'x', '*']\n","\n","# 3D plot\n","fig = plt.figure(figsize = (25,10))\n","#fig = plt.figure()\n","ax = fig.add_subplot(projection='3d')\n","\n","for c, category in zip(col, categories):\n","    xs = X_3red[X['category_name'] == category].T[0]\n","    ys = X_3red[X['category_name'] == category].T[1]\n","    zs = X_3red[X['category_name'] == category].T[2]\n","\n","    #ax.scatter(xs, ys, c = c, marker='o')\n","    ax.scatter(xs, ys, zs, c=c, marker='^')\n","\n","#ax.grid(color='grey', linestyle=':', linewidth=2, alpha=0.2)\n","ax.set_xlabel('\\nX Label')\n","ax.set_ylabel('\\nY Label')\n","ax.set_zlabel('\\nZ Label')\n","\n","plt.show()"]}],"metadata":{"colab":{"collapsed_sections":["PQPCUbx1ie4R","8qg4up1B_EhD","lC7ymUlG_fai","xtvWLH1x_7nV","bgULadKFBXL-","SZ4rgA1mBir5","9VirxMl6CGN2","yoTS9Vh8ESzB","NGVM3wSjFt7v","bH9BQLSWF9Uf","y8I6L8Z8JGsv","TFIl1hpMJqnv","DobYRQ4FLetu","9pfemrkcLiUG"],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
